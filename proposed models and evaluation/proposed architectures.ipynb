{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff9350bc-e58a-46a8-af1d-65e5d342381e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% [code]\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, log_loss, matthews_corrcoef, confusion_matrix\n",
    ")\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from pandas.plotting import parallel_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b98dcb88-c69a-45d4-87de-d0a835b584c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress:   0%|                                                                          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress:  11%|███████▏                                                         | 1/9 [05:04<40:39, 304.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress:  22%|██████████████▍                                                  | 2/9 [09:46<33:57, 291.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress:  33%|█████████████████████▋                                           | 3/9 [14:17<28:11, 281.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress:  44%|███████████████████████████▌                                  | 4/9 [54:48<1:34:12, 1130.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress:  56%|█████████████████████████████████▎                          | 5/9 [1:34:28<1:45:23, 1580.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress:  67%|████████████████████████████████████████                    | 6/9 [2:14:00<1:32:29, 1849.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress:  78%|████████████████████████████████████████████████▏             | 7/9 [2:21:10<46:11, 1385.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress:  89%|███████████████████████████████████████████████████████       | 8/9 [2:28:32<18:05, 1085.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress: 100%|██████████████████████████████████████████████████████████████| 9/9 [2:35:40<00:00, 1037.86s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Ensure plot directory exists\n",
    "os.makedirs(\"plot\", exist_ok=True)\n",
    "\n",
    "#---------------------------\n",
    "# Custom EarlyStopping\n",
    "#---------------------------\n",
    "class ConsecutiveEarlyStopping(Callback):\n",
    "    def __init__(self, monitor='accuracy', threshold=0.99, patience=5, verbose=1):\n",
    "        super().__init__()\n",
    "        self.monitor = monitor\n",
    "        self.threshold = threshold\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val = logs.get(self.monitor)\n",
    "        if val is None:\n",
    "            return\n",
    "        if val > self.threshold:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                if self.verbose:\n",
    "                    print(f\"\\nEpoch {epoch+1}: {self.monitor} > {self.threshold}\"\n",
    "                          f\" for {self.patience} consecutive epochs. Stopping.\")\n",
    "                self.model.stop_training = True\n",
    "        else:\n",
    "            self.counter = 0\n",
    "\n",
    "#---------------------------\n",
    "# Build Models\n",
    "#---------------------------\n",
    "def build_cnn_model():\n",
    "    inputs = tf.keras.Input(shape=(100, 6))\n",
    "    x = layers.Conv1D(32, 3, activation=\"relu\")(inputs)\n",
    "    x = layers.Conv1D(64, 3, activation=\"relu\")(x)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = layers.Conv1D(128, 3, activation=\"relu\")(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = models.Model(inputs=inputs, outputs=outputs, name=\"CNN\")\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.005),\n",
    "                  loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_lstm_model():\n",
    "    inputs = tf.keras.Input(shape=(100, 6))\n",
    "    x = layers.LSTM(64, return_sequences=True)(inputs)\n",
    "    x = layers.LSTM(32)(x)\n",
    "    x = layers.Dense(32, activation=\"relu\")(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = models.Model(inputs=inputs, outputs=outputs, name=\"LSTM\")\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.005),\n",
    "                  loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_cnn_lstm_model():\n",
    "    inputs = tf.keras.Input(shape=(100, 6))\n",
    "    x = layers.Conv1D(32, 3, activation=\"relu\")(inputs)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = layers.Conv1D(64, 3, activation=\"relu\")(x)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = layers.LSTM(32)(x)\n",
    "    x = layers.Dense(32, activation=\"relu\")(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = models.Model(inputs=inputs, outputs=outputs, name=\"CNN_LSTM\")\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.005),\n",
    "                  loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "model_builders = [\n",
    "    (\"CNN\", build_cnn_model),\n",
    "    (\"LSTM\", build_lstm_model),\n",
    "    (\"CNN_LSTM\", build_cnn_lstm_model)\n",
    "]\n",
    "\n",
    "#---------------------------\n",
    "# Load and Prepare Data\n",
    "#---------------------------\n",
    "def load_and_prepare():\n",
    "    # Load pre-saved CSVs\n",
    "    df_feat = pd.read_csv(\"final_f.csv\")\n",
    "    df_lab  = pd.read_csv(\"final_l.csv\")\n",
    "    # Handle missing values\n",
    "    if df_feat.isnull().values.any():\n",
    "        df_feat.fillna(df_feat.mean(), inplace=True)\n",
    "    if df_lab.isnull().values.any():\n",
    "        df_lab.fillna(df_lab.mean(), inplace=True)\n",
    "    # Reshape\n",
    "    rows_per = 100\n",
    "    n_feat = df_feat.shape[1]\n",
    "    n_samples = df_lab.shape[0]\n",
    "    X = df_feat.values.reshape(n_samples, rows_per, n_feat)\n",
    "    y = df_lab.iloc[:,0].values\n",
    "    return X, y\n",
    "\n",
    "#---------------------------\n",
    "# Scale Data\n",
    "#---------------------------\n",
    "def scale_data(X):\n",
    "    n_samples, rows, n_feat = X.shape\n",
    "    scaler = MinMaxScaler((0,1))\n",
    "    X_flat = X.reshape(-1, n_feat)\n",
    "    Xs_flat = scaler.fit_transform(X_flat)\n",
    "    with open(\"scaler1.json\", \"w\") as f:\n",
    "        json.dump({\n",
    "            \"min_\": scaler.min_.tolist(),\n",
    "            \"scale_\": scaler.scale_.tolist(),\n",
    "            \"data_min_\": scaler.data_min_.tolist(),\n",
    "            \"data_max_\": scaler.data_max_.tolist(),\n",
    "            \"data_range_\": scaler.data_range_.tolist()\n",
    "        }, f, indent=4)\n",
    "    return Xs_flat.reshape(n_samples, rows, n_feat)\n",
    "\n",
    "#---------------------------\n",
    "# Main Workflow\n",
    "#---------------------------\n",
    "def main():\n",
    "    # Load & scale\n",
    "    X, y = load_and_prepare()\n",
    "    Xs = scale_data(X)\n",
    "    # Training settings\n",
    "    iterations = 3\n",
    "    records = []\n",
    "    total = len(model_builders) * iterations\n",
    "    pbar = tqdm(total=total, desc=\"Overall Progress\")\n",
    "\n",
    "    for name, builder in model_builders:\n",
    "        model = builder()\n",
    "        for it in range(iterations):\n",
    "            try:\n",
    "                # Train-test split\n",
    "                X_tr, X_temp, y_tr, y_temp = train_test_split(Xs, y, test_size=0.2, random_state=it)\n",
    "                X_val, X_te, y_val, y_te = train_test_split(X_temp, y_temp, test_size=0.5, random_state=it)\n",
    "                # Train\n",
    "                model.fit(\n",
    "                    X_tr, y_tr,\n",
    "                    epochs=100,\n",
    "                    batch_size=16,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks=[ConsecutiveEarlyStopping()],\n",
    "                    verbose=0\n",
    "                )\n",
    "                # Evaluate\n",
    "                prob = model.predict(X_te)\n",
    "                pred = (prob > 0.5).astype(int).flatten()\n",
    "                tn, fp, fn, tp = confusion_matrix(y_te, pred).ravel()\n",
    "                records.append({\n",
    "                    'model': name,\n",
    "                    'accuracy': accuracy_score(y_te, pred),\n",
    "                    'precision': precision_score(y_te, pred, zero_division=0),\n",
    "                    'recall': recall_score(y_te, pred, zero_division=0),\n",
    "                    'f1_score': f1_score(y_te, pred, zero_division=0),\n",
    "                    'roc_auc': roc_auc_score(y_te, prob) if len(np.unique(y_te))>1 else np.nan,\n",
    "                    'mcc': matthews_corrcoef(y_te, pred),\n",
    "                    'specificity': tn/(tn+fp) if (tn+fp)>0 else np.nan,\n",
    "                    'log_loss': log_loss(y_te, prob)\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"{name} iteration {it} failed: {e}\")\n",
    "            pbar.update(1)\n",
    "    pbar.close()\n",
    "\n",
    "    # Aggregate and save\n",
    "    df = pd.DataFrame(records)\n",
    "    metrics = ['accuracy','precision','recall','f1_score','roc_auc','mcc','specificity','log_loss']\n",
    "    agg = df.groupby('model')[metrics].mean().reset_index()\n",
    "    agg.to_csv('eval.csv', index=False)\n",
    "\n",
    "    # Plotting aggregated\n",
    "    agg['inv_log_loss'] = 1/(1+agg['log_loss'])\n",
    "    labels = ['accuracy','precision','recall','f1_score','roc_auc','mcc','specificity','inv_log_loss']\n",
    "    # Heatmap\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(agg.set_index('model')[labels], annot=True, cmap='YlGnBu', fmt='.3f')\n",
    "    plt.title('Heatmap of Average Metrics per Model')\n",
    "    plt.savefig('plot/heatmap_avg.png'); plt.close()\n",
    "    # Radar Chart\n",
    "    angles = np.linspace(0,2*np.pi,len(labels),endpoint=False).tolist(); angles += angles[:1]\n",
    "    fig, ax = plt.subplots(figsize=(8,8), subplot_kw=dict(polar=True))\n",
    "    for _,row in agg.iterrows():\n",
    "        vals = row[labels].tolist(); vals += vals[:1]\n",
    "        ax.plot(angles, vals, label=row['model'])\n",
    "        ax.fill(angles, vals, alpha=0.1)\n",
    "    ax.set_xticks(angles[:-1]); ax.set_xticklabels(labels)\n",
    "    ax.legend(bbox_to_anchor=(1.1,1.1)); plt.savefig('plot/radar_avg.png'); plt.close()\n",
    "    # Parallel Coordinates\n",
    "    pc = agg.copy(); pc['model_label'] = pc['model']\n",
    "    plt.figure(figsize=(8,6))\n",
    "    parallel_coordinates(pc[['model_label']+labels], 'model_label')\n",
    "    plt.title('Parallel Coordinates of Average Metrics')\n",
    "    plt.savefig('plot/parallel_avg.png'); plt.close()\n",
    "    # Grouped Bar\n",
    "    melt = agg.melt(id_vars='model', value_vars=labels, var_name='metric', value_name='value')\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.barplot(data=melt, x='metric', y='value', hue='model')\n",
    "    plt.title('Grouped Bar Chart of Average Metrics')\n",
    "    plt.savefig('plot/grouped_bar_avg.png'); plt.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "# -- End of Notebook Cell --\n",
    "\n",
    "# What this code does:\n",
    "# - Loads `final_f.csv` and `final_l.csv`, fills missing values with column means.\n",
    "# - Reshapes data into (samples, 100, features) and scales features with MinMaxScaler.\n",
    "# - Initializes three models: CNN, LSTM, CNN_LSTM.\n",
    "# - Trains each model for 3 iterations (100 epochs, early stopping at 99% accuracy for 5 epochs) with a progress bar.\n",
    "# - Collects accuracy, precision, recall, F1, ROC AUC, MCC, specificity, and log loss per iteration.\n",
    "# - Aggregates metrics by model (averaged over iterations), saves `eval.csv`.\n",
    "# - Plots four comparison charts (heatmap, radar, parallel coordinates, grouped bar) from the averaged metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde24748-375e-4110-9757-fec848150a49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
