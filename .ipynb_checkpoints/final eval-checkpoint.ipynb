{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9350bc-e58a-46a8-af1d-65e5d342381e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% [code]\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, log_loss, matthews_corrcoef, confusion_matrix\n",
    ")\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, LSTM, GRU, SimpleRNN, Conv1D, MaxPooling1D,\n",
    "    GlobalAveragePooling1D, Dense, UpSampling1D, Concatenate, Bidirectional\n",
    ")\n",
    "from pandas.plotting import parallel_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98dcb88-c69a-45d4-87de-d0a835b584c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ensure plot directory exists\n",
    "os.makedirs(\"plot\", exist_ok=True)\n",
    "\n",
    "#---------------------------\n",
    "# Custom EarlyStopping\n",
    "#---------------------------\n",
    "class ConsecutiveEarlyStopping(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, monitor='accuracy', threshold=0.99, patience=5, verbose=1):\n",
    "        super().__init__()\n",
    "        self.monitor = monitor\n",
    "        self.threshold = threshold\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val = logs.get(self.monitor)\n",
    "        if val is None:\n",
    "            return\n",
    "        if val > self.threshold:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                if self.verbose:\n",
    "                    print(f\"\\nEpoch {epoch+1}: {self.monitor} > {self.threshold}\"\n",
    "                          f\" for {self.patience} consecutive epochs. Stopping.\")\n",
    "                self.model.stop_training = True\n",
    "        else:\n",
    "            self.counter = 0\n",
    "\n",
    "#---------------------------\n",
    "# Data Loading & Preprocessing\n",
    "#---------------------------\n",
    "def load_and_preprocess():\n",
    "    df_feat = pd.read_csv(\"cdf.csv\")\n",
    "    df_lab  = pd.read_csv(\"cdl.csv\")\n",
    "    rows_per = 100\n",
    "    n_feat   = df_feat.shape[1]\n",
    "    n_samp   = df_feat.shape[0] // rows_per\n",
    "\n",
    "    X_flat, y = [], []\n",
    "    for i in range(n_samp):\n",
    "        block = df_feat.iloc[i*rows_per:(i+1)*rows_per].values\n",
    "        X_flat.append(block.flatten())\n",
    "        y.append(df_lab.iloc[i,0])\n",
    "    X_flat = np.array(X_flat)\n",
    "    y      = np.array(y)\n",
    "\n",
    "    # SMOTE balancing\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_res, y_res = sm.fit_resample(X_flat, y)\n",
    "    n_new = X_res.shape[0]\n",
    "\n",
    "    # Save reshaped feature CSV (100x6 blocks stacked)\n",
    "    X_blocks = X_res.reshape(n_new, rows_per, n_feat)\n",
    "    X2 = X_blocks.reshape(n_new*rows_per, n_feat)\n",
    "    pd.DataFrame(X2, columns=df_feat.columns).to_csv(\"final_f.csv\", index=False)\n",
    "    pd.DataFrame(y_res, columns=[\"label\"]).to_csv(\"final_l.csv\", index=False)\n",
    "\n",
    "    return X_res, y_res, rows_per, n_feat\n",
    "\n",
    "#---------------------------\n",
    "# Feature Scaling\n",
    "#---------------------------\n",
    "def scale_features(X):\n",
    "    scaler = MinMaxScaler((0,1))\n",
    "    Xs = scaler.fit_transform(X)\n",
    "    params = {\n",
    "        \"min_\": scaler.min_.tolist(),\n",
    "        \"scale_\": scaler.scale_.tolist(),\n",
    "        \"data_min_\": scaler.data_min_.tolist(),\n",
    "        \"data_max_\": scaler.data_max_.tolist(),\n",
    "        \"data_range_\": scaler.data_range_.tolist()\n",
    "    }\n",
    "    with open(\"scaler1.json\",\"w\") as f:\n",
    "        json.dump(params, f, indent=4)\n",
    "    return Xs\n",
    "\n",
    "#---------------------------\n",
    "# Model Factory (15 variants)\n",
    "#---------------------------\n",
    "def create_model(idx, input_shape):\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.005)\n",
    "    # Sequential models\n",
    "    if idx == 0:\n",
    "        m = Sequential(name=\"LSTM_Basic\")\n",
    "        m.add(LSTM(32, return_sequences=True, input_shape=input_shape))\n",
    "        m.add(LSTM(16))\n",
    "    elif idx == 1:\n",
    "        m = Sequential(name=\"GRU_Basic\")\n",
    "        m.add(GRU(32, return_sequences=True, input_shape=input_shape))\n",
    "        m.add(GRU(16))\n",
    "    elif idx == 2:\n",
    "        m = Sequential(name=\"RNN_Basic\")\n",
    "        m.add(SimpleRNN(32, return_sequences=True, input_shape=input_shape))\n",
    "        m.add(SimpleRNN(16))\n",
    "    elif idx == 3:\n",
    "        m = Sequential(name=\"CNN_1D\")\n",
    "        m.add(Conv1D(32, 3, activation='relu', input_shape=input_shape))\n",
    "        m.add(MaxPooling1D(2))\n",
    "        m.add(Conv1D(64, 3, activation='relu'))\n",
    "        m.add(GlobalAveragePooling1D())\n",
    "    elif idx == 4:\n",
    "        m = Sequential(name=\"CNN_LSTM\")\n",
    "        m.add(Conv1D(32, 3, activation='relu', input_shape=input_shape))\n",
    "        m.add(MaxPooling1D(2))\n",
    "        m.add(LSTM(32))\n",
    "    elif idx == 5:\n",
    "        m = Sequential(name=\"CNN_GRU\")\n",
    "        m.add(Conv1D(32, 3, activation='relu', input_shape=input_shape))\n",
    "        m.add(MaxPooling1D(2))\n",
    "        m.add(GRU(32))\n",
    "    elif idx == 6:\n",
    "        m = Sequential(name=\"CNN_RNN\")\n",
    "        m.add(Conv1D(32, 3, activation='relu', input_shape=input_shape))\n",
    "        m.add(MaxPooling1D(2))\n",
    "        m.add(SimpleRNN(32))\n",
    "    elif idx == 7:\n",
    "        m = Sequential(name=\"Deeper_CNN\")\n",
    "        m.add(Conv1D(32,3,activation='relu',input_shape=input_shape))\n",
    "        m.add(Conv1D(32,3,activation='relu'))\n",
    "        m.add(MaxPooling1D(2))\n",
    "        m.add(Conv1D(64,3,activation='relu'))\n",
    "        m.add(MaxPooling1D(2))\n",
    "        m.add(GlobalAveragePooling1D())\n",
    "    elif idx == 8:\n",
    "        m = Sequential(name=\"Stacked_LSTM\")\n",
    "        m.add(LSTM(64,return_sequences=True,input_shape=input_shape))\n",
    "        m.add(LSTM(64,return_sequences=True))\n",
    "        m.add(LSTM(32))\n",
    "    elif idx == 9:\n",
    "        m = Sequential(name=\"Stacked_GRU\")\n",
    "        m.add(GRU(64,return_sequences=True,input_shape=input_shape))\n",
    "        m.add(GRU(64,return_sequences=True))\n",
    "        m.add(GRU(32))\n",
    "    elif idx == 10:\n",
    "        m = Sequential(name=\"CNN_BiLSTM\")\n",
    "        m.add(Conv1D(32, 3, activation='relu', input_shape=input_shape))\n",
    "        m.add(MaxPooling1D(2))\n",
    "        m.add(Bidirectional(LSTM(32)))\n",
    "    elif idx == 11:\n",
    "        m = Sequential(name=\"CNN_BiGRU\")\n",
    "        m.add(Conv1D(32, 3, activation='relu', input_shape=input_shape))\n",
    "        m.add(MaxPooling1D(2))\n",
    "        m.add(Bidirectional(GRU(32)))\n",
    "    elif idx == 14:\n",
    "        m = Sequential(name=\"FCN\")\n",
    "        m.add(Conv1D(128,8,activation='relu',padding='same',input_shape=input_shape))\n",
    "        m.add(Conv1D(256,5,activation='relu',padding='same'))\n",
    "        m.add(Conv1D(128,3,activation='relu',padding='same'))\n",
    "        m.add(GlobalAveragePooling1D())\n",
    "    # Functional models\n",
    "    elif idx == 12:\n",
    "        inp = Input(shape=input_shape)\n",
    "        c1 = Conv1D(16,3,activation='relu',padding='same')(inp)\n",
    "        p1 = MaxPooling1D(2)(c1)\n",
    "        c2 = Conv1D(32,3,activation='relu',padding='same')(p1)\n",
    "        p2 = MaxPooling1D(2)(c2)\n",
    "        bn = Conv1D(64,3,activation='relu',padding='same')(p2)\n",
    "        u2 = UpSampling1D(2)(bn)\n",
    "        cat2 = Concatenate()([u2, c2])\n",
    "        c3 = Conv1D(32,3,activation='relu',padding='same')(cat2)\n",
    "        u1 = UpSampling1D(2)(c3)\n",
    "        cat1 = Concatenate()([u1, c1])\n",
    "        c4 = Conv1D(16,3,activation='relu',padding='same')(cat1)\n",
    "        gap = GlobalAveragePooling1D()(c4)\n",
    "        out = Dense(1, activation='sigmoid')(gap)\n",
    "        m = Model(inputs=inp, outputs=out, name=\"UNet_1D\")\n",
    "    elif idx == 13:\n",
    "        inp = Input(shape=input_shape)\n",
    "        x = Conv1D(32,3,activation='relu',padding='same')(inp)\n",
    "        sc = x\n",
    "        x = Conv1D(32,3,activation='relu',padding='same')(x)\n",
    "        x = layers.Add()([x, sc])\n",
    "        x = MaxPooling1D(2)(x)\n",
    "        x = GlobalAveragePooling1D()(x)\n",
    "        out = Dense(1, activation='sigmoid')(x)\n",
    "        m = Model(inputs=inp, outputs=out, name=\"ResNet_1D\")\n",
    "    else:\n",
    "        raise ValueError(\"Model index must be between 0 and 14.\")\n",
    "\n",
    "    # Add final dense for sequential\n",
    "    if isinstance(m, Sequential):\n",
    "        m.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    m.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return m\n",
    "\n",
    "#---------------------------\n",
    "# Training & Metrics Collection\n",
    "#---------------------------\n",
    "def train_and_collect(models, X, y, rows, n_feat):\n",
    "    records = []\n",
    "    for iteration in range(20):\n",
    "        X_seq = X.reshape(-1, rows, n_feat)\n",
    "        X_tr, X_temp, y_tr, y_temp = train_test_split(X_seq, y, test_size=0.2, random_state=iteration)\n",
    "        X_val, X_te, y_val, y_te = train_test_split(X_temp, y_temp, test_size=0.5, random_state=iteration)\n",
    "        for idx, model in enumerate(models):\n",
    "            cb = ConsecutiveEarlyStopping()\n",
    "            model.fit(\n",
    "                X_tr, y_tr,\n",
    "                epochs=200,\n",
    "                batch_size=16,\n",
    "                validation_data=(X_val, y_val),\n",
    "                callbacks=[cb],\n",
    "                verbose=0\n",
    "            )\n",
    "            prob = model.predict(X_te)\n",
    "            pred = (prob > 0.5).astype(int).flatten()\n",
    "            tn, fp, fn, tp = confusion_matrix(y_te, pred).ravel()\n",
    "            records.append({\n",
    "                'model_index': idx,\n",
    "                'accuracy': accuracy_score(y_te, pred),\n",
    "                'precision': precision_score(y_te, pred, zero_division=0),\n",
    "                'recall': recall_score(y_te, pred, zero_division=0),\n",
    "                'f1_score': f1_score(y_te, pred, zero_division=0),\n",
    "                'roc_auc': roc_auc_score(y_te, prob) if len(np.unique(y_te))>1 else np.nan,\n",
    "                'mcc': matthews_corrcoef(y_te, pred),\n",
    "                'specificity': tn/(tn+fp) if (tn+fp)>0 else np.nan,\n",
    "                'log_loss': log_loss(y_te, prob)\n",
    "            })\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "#---------------------------\n",
    "# Aggregation & Plotting\n",
    "#---------------------------\n",
    "def aggregate_and_save(df):\n",
    "    metrics = ['accuracy','precision','recall','f1_score','roc_auc','mcc','specificity','log_loss']\n",
    "    agg = df.groupby('model_index')[metrics].mean().reset_index()\n",
    "    agg.to_csv('eval.csv', index=False)\n",
    "    return agg\n",
    "\n",
    "def plot_aggregated(agg):\n",
    "    agg['inv_log_loss'] = 1/(1+agg['log_loss'])\n",
    "    labels = ['accuracy','precision','recall','f1_score','roc_auc','mcc','specificity','inv_log_loss']\n",
    "    # Heatmap\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.heatmap(agg.set_index('model_index')[labels], annot=True, cmap='YlGnBu', fmt='.3f')\n",
    "    plt.title('Heatmap of Average Metrics per Model')\n",
    "    plt.savefig('plot/heatmap_avg.png')\n",
    "    plt.close()\n",
    "    # Radar Chart\n",
    "    angles = np.linspace(0,2*np.pi,len(labels),endpoint=False).tolist(); angles += angles[:1]\n",
    "    fig, ax = plt.subplots(figsize=(8,8), subplot_kw=dict(polar=True))\n",
    "    for _,row in agg.iterrows():\n",
    "        vals = row[labels].tolist(); vals += vals[:1]\n",
    "        ax.plot(angles, vals, label=f\"Model {int(row['model_index'])}\")\n",
    "        ax.fill(angles, vals, alpha=0.1)\n",
    "    ax.set_xticks(angles[:-1]); ax.set_xticklabels(labels)\n",
    "    ax.set_title('Radar Chart of Average Metrics per Model')\n",
    "    ax.legend(bbox_to_anchor=(1.1,1.1)); plt.savefig('plot/radar_avg.png'); plt.close()\n",
    "    # Parallel Coordinates\n",
    "    pc = agg.copy(); pc['model'] = pc['model_index'].astype(str)\n",
    "    plt.figure(figsize=(12,6))\n",
    "    parallel_coordinates(pc[['model']+labels],'model'); plt.title('Parallel Coordinates Plot');\n",
    "    plt.savefig('plot/parallel_avg.png'); plt.close()\n",
    "    # Grouped Bar\n",
    "    melt = agg.melt(id_vars='model_index',value_vars=labels,var_name='metric',value_name='value')\n",
    "    plt.figure(figsize=(12,6))\n",
    "    sns.barplot(data=melt, x='metric', y='value', hue='model_index'); plt.title('Grouped Bar Chart')\n",
    "    plt.savefig('plot/grouped_bar_avg.png'); plt.close()\n",
    "\n",
    "#---------------------------\n",
    "# Main Execution\n",
    "#---------------------------\n",
    "def main():\n",
    "    X, y, rows, n_feat = load_and_preprocess()\n",
    "    print('Data loaded and preprocessed.')\n",
    "    Xs = scale_features(X); print('Features scaled.')\n",
    "    models = [create_model(i, (rows, n_feat)) for i in range(15)]; print('Models initialized.')\n",
    "    df_metrics = train_and_collect(models, Xs, y, rows, n_feat); print('Training complete.')\n",
    "    agg = aggregate_and_save(df_metrics); print('Aggregated metrics saved.')\n",
    "    plot_aggregated(agg); print('Plots saved in plot/')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "#%% [markdown]\n",
    "# Instructions:\n",
    "# 1. Ensure `cdf.csv` and `cdl.csv` exist.\n",
    "# 2. Run this cell in Jupyter.\n",
    "# 3. Check outputs in working directory and `plot/` folder.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
